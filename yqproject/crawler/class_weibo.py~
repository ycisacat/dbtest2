# coding=utf-8

import sys
from class_headhunter import *
from create_file import *
from crawler.class_save_data import *
reload(sys)
sys.setdefaultencoding('utf-8')
__author__ = 'gu'


class WeiboPage():
    """
    本类爬取新浪微博用户的博文及评论
    """

    def __init__(self, one_user):
        """
        :param one_user: 用户的id，即是猎头的id
         用户首页的源代码：homepage
        :return:homepage
        """
        self.one_user = one_user
        self.weibo_list = []
        self.time_list = []
        self.weibo = []
        self.writing_time = []
        self.base_page = []
        self.dictwt = {}
        self.no_repeat_list = []
        self.hunter_text_dir = os.path.join(BASE_DIR, 'documents', 'hunter_text/')
        self.comment_dir = os.path.join(BASE_DIR, 'documents', 'comment/')  # 猎头微博的评论内容
        self.forward_path_dir = os.path.join(BASE_DIR, 'documents', 'forward_path/')  # 猎头的微博的转发路径
        self.header = {'User-Agent': 'Mozilla/' + str(
            float(int(random.uniform(1, 6)))) + '(X11; Ubuntu; Linux i686; rv:35.0) Gecko/20100101 Firefox/' + str(
            float(int(random.uniform(29, 36))))}
        self.blog_id =''
        self.post_time = ''
        self.event_id = ''
        self.corpus_dir = ''

    def get_page(self, p1, p2):
        """
        调用全部方法，相当于控制器

        爬取博文的方法 one_id_text()
        保存猎头当天发表的博文和博转评方法：write_text()
        爬取猎头微博的评论内容： get_comment()
        爬取猎头的微博的转发路径方法： get_forward_path()
        搜索话题爬取话题相关微博的路径方法： get_topic()
        :param p1: 用户博文的开始页
        :param p2: 用户博文的结束页
        :return:
        """

        try:
            print '正在爬取用户', self.one_user, '相关信息,开始时间:', time.time()
            one = self.one_id_text(self.one_user, p1, p2)  # 字典{id:所有内容}
            print "进入下一阶段"
            print len(one.items())
            if len(one.items()) != 0:
                print "safsdfa"
                self.write_hunter_txt(one.keys(), one.values())
                # self.get_comment(one.keys(), one.values())
                # self.get_forward_path(one.keys(), one.values())
                all_topic = self.get_topic(one.keys(), one.values())

                for one_topic in all_topic:
                    a = multiprocessing.Process(target=self.get_search_topic, args=(one.keys(), one_topic))
                    a.start()
                a.join()

                # self.get_search_topic(one.keys(),one_topic)  # 搜索每一个话题路径
            self.dictwt.clear()  # 清空一个用户的信息,准备开始下一个用户
        except AttributeError:
            print "get page error"
        return True

    def one_id_text(self, i, a, b):
        """
        爬取用户博文的方法
        :param i: 用户的id
        :param a: 博文的开始页
        :param b: 博文的结束页
        :return: 返回 all_dict ，键为今天发表了博文用户的id，值为这个id的博文,时间,赞,转发链接 转发,评论链接 评论 组成的元组列表
        """
        all_dict = {}
        writing_time = []
        weibo = []  # 存放一个人的所有博文
        weibo_id = []
        praise = []  # 赞
        forward = []  # 转发
        comment = []  # 评论
        forward_url = []
        comment_url = []
        blog_origin = []

        try:
            host_url = "http://weibo.cn/u/" + str(i)
            url_request = urllib2.Request(host_url, headers=self.header)
            response = urllib2.urlopen(url_request, timeout=30)
            text = response.read()
            page_num = re.compile('跳页" />.*?/(.*?)页')  # 匹配微博页数
            num = page_num.findall(text)

            for nm in num:  # 判断页数,不足b页时到pm页为止
                pm = int(nm)
                if b > pm:
                    b = pm
                else:
                    pass

            for k in xrange(a, b):  # 每一页的博文获取
                if k % 5 == 4:
                    time.sleep(random.randint(0, 5))
                else:
                    pass
                print "第", k, "页"

                url = "http://weibo.cn/u/" + str(self.one_user) + "?page=" + str(k)  # 猎头的首页
                req = urllib2.Request(url=url, headers=self.header)
                self.homepage = urllib2.urlopen(req).read()

                base_patterns = re.compile(
                    'class="c" id="(.*?)">.*?<span class="ctt">(.*?)</span>.*?>赞\[(\d+)]</a>&nbsp;<a href="(.*?)">转发\[(\d+)]</a>&nbsp;<a href="(.*?)" class="cc">评论\[(\d+)]</a>.*?<span class="ct">(.*?)&nbsp',
                    re.M)  # 匹配博文id 、博文 点赞 转发链接 转发 评论链接 评论 时间
                self.base_page = base_patterns.findall(self.homepage)
                if len(self.base_page) > 0:
                    print "登陆成功"
                else:
                    print "登陆失败"
                self.add_text()

            print "sda", len(self.dictwt.keys())

            for id in self.dictwt.keys():
                print id

            if len(self.dictwt) > 0:
                print "用户 ", i, "今天有发表博文"
                for item0, item1 in self.dictwt.items():  # item0为tu[0],键=博文id. item1 = 值
                    print "bianli", item0
                    print 'lllllllll'
                    weibo_id.append(item0)  # 键=博文id
                    wbtime = self.cleaned_wbtime(item1[6])  # 清理博文时间

                    blog_origin_patternts = re.compile('>(http://t.cn/.*?)</a></span>')
                    blog_origin_one = blog_origin_patternts.findall(item1[0])  # 爬取新闻链接
                    if len(blog_origin_one) == 0:
                        blog_origin.append(str(None))
                    else:
                        blog_origin.append(blog_origin_one[0])

                    content = self.cleaned_weibo(item1[0])  # 清理博文
                    writing_time.append(wbtime)  # 存放时间
                    weibo.append(content)  # 存放博文
                    praise.append(item1[1])  # 点赞量
                    forward_url.append(item1[2])  # 转发量url
                    forward.append(item1[3])
                    comment_url.append(item1[4])  # 评论量url
                    comment.append(item1[5])
                    print "博文id", item0
                    print "博文", item1[0]
                    print "点赞", item1[1]
                    print "转发", item1[3]
                    print "评论", item1[5]
                    print '转发链接', item1[2]
                    print '评论链接', item1[4]
                    # print 'weibo', weibo
                    # print '时间', writing_time
                    # print '链接', blog_origin

            else:
                print "用户", i, "今天没有发表博文"

            if len(weibo) > 0:
                # 各种列表打包
                tw = zip(writing_time, weibo_id, weibo, praise, forward_url, forward, comment_url, comment, blog_origin)
                all_dict.setdefault(i, tw)
        except:
            pass
        return all_dict  # 返回字典，键为有动态的id，值为这个id的博文,时间,赞,转发,评论,博文链接组成的元组列表

    # 把爬取到的内容加入字典
    def add_text(self):
        """
        判断爬取的博文是否为今天发表的，如果是则保存在字典里
        :return:  self.dictwt 键为博文，值为博文相关的属性
        """
        # 匹配博文id 、博文 点赞 转发链接 转发 评论链接 评论 时间
        for tu in self.base_page:  # tu:每一条博文块
            # 判断是否今天博文
            if (re.match('今天', tu[7])):  # tu[6]里找到标志为今天的时间
                if len(tu[7]) > 0:
                    # print "这是今天的博文"
                    self.dictwt.setdefault(tu[0], []).append(tu[1])
                    self.dictwt.setdefault(tu[0], []).append(tu[2])  # 键为tu[0],即是博文id,tu[1]为没处理的博文
                    self.dictwt.setdefault(tu[0], []).append(tu[3])
                    self.dictwt.setdefault(tu[0], []).append(tu[4])  # tu[2]点赞数,tu[3]为转发链接,tu[4]为转发数量
                    self.dictwt.setdefault(tu[0], []).append(tu[5])
                    self.dictwt.setdefault(tu[0], []).append(tu[6])  # tu[5]是评论链接 tu[6]为评论数 tu[7]是时间
                    self.dictwt.setdefault(tu[0], []).append(tu[7])
                else:
                    pass  # 匹配博文 点赞 转发链接 转发 评论链接 评论 时间

            elif (re.search('\d+分钟前', tu[7])):  # tu[6]里找到标志为几分钟前的时间
                if len(tu[7]) > 0:
                    # print "这是今天的博文"
                    self.dictwt.setdefault(tu[0], []).append(tu[1])
                    self.dictwt.setdefault(tu[0], []).append(tu[2])  # 键为tu[0],即是博文id,tu[1]没处理的博文
                    self.dictwt.setdefault(tu[0], []).append(tu[3])
                    self.dictwt.setdefault(tu[0], []).append(tu[4])  # tu[2]点赞数,tu[3]为转发链接,tu[4]为转发数量
                    self.dictwt.setdefault(tu[0], []).append(tu[5])
                    self.dictwt.setdefault(tu[0], []).append(tu[6])  # tu[5]是评论链接 tu[6]为评论数 tu[7]是时间
                    self.dictwt.setdefault(tu[0], []).append(tu[7])
                else:
                    pass
            else:
                print "这不是今天的博文"

        print 'len', len(self.dictwt)

        return self.dictwt

    def cleaned_wbtime(self, item1):
        """
        转化时间格式
        :param item1: 时间
        :return:标准格式的时间
        """
        extra1 = re.compile('</span>.*$')  # 匹配输入字符串的结束位置
        today = re.compile('今天')
        ago = re.compile('\d+分钟前')
        wbtime = re.sub(extra1, ' ', item1)  #
        t = time.strftime('%m' + '月' + '%d' + '日', time.localtime())
        t = t.decode('utf-8')
        wbtime = re.sub(today, t, wbtime)
        wbtime = re.sub(ago, t, wbtime)  # wbtime,gb2312,type str
        return wbtime

    def cleaned_weibo(self, item0):
        """
        净化博文
        :param item0:博文
        :return:净化干净的博文
        """
        sub_title = re.compile('<img.*?注')
        tag = re.compile('<.*?>')  # 去除标签
        link = re.compile('<a href=.*?>|http.*?</a>')  # 去除链接
        content = re.sub(link, " ", item0)
        content = re.sub(tag, '', content)
        content = re.sub(sub_title, '', content)
        # content = content.encode('utf-8', 'ignore')  # content为完成的博文
        return content

    def write_hunter_txt(self, key, value):
        """
        写猎头的博文txt备份,在get_page()中启动或禁用
        :param key: 字典的键，即是id
        :param value: 值为(时间,博文,点赞,转发链接,转发量,评论链接,评论量）元组组成的列表
        :return:
        """
        print '正在写入用户', key, '博文'
        for i in value:
            print "value", value
            print key[0], "今天发的博文数量", len(i)
            if os.path.exists(self.hunter_text_dir):
                pass
            else:
                os.mkdir(self.hunter_text_dir)
            save_file = open(self.hunter_text_dir + 'uid=' + str(key[0]) + '.txt', 'w+')
            print "创建文件成功"
            for j in i:
                print "id,时间,博文id,博文,点赞,转发链接,转发量,评论链接,评论量,新闻源", key[0], j[0], j[1], j[2], j[3], j[4], j[5], j[6], j[7], \
                    j[8]
                save_file.write(
                    str(key[0])
                    + "\n博文id: " + str(j[0])
                    + "\n时间： " + str(j[1])
                    + "\n博文： " + str(j[2])
                    + "\n点赞量： " + str(j[3])
                    + "\n转发链接： " + str(j[4])
                    + "\n转发量： " + str(j[5])
                    + "\n评论链接 ： " + str(j[6])
                    + "\n评论量： " + str(j[7])
                    + "\n新闻源： " + str(j[8])
                    + '\n\n')
            save_file.close()


        print '完成用户', key, '的博文存储'

    def get_topic(self, key, value):
        """
        爬取博文的主题，【】内的内容
        :param key:
        :param value:
        :return: self.weibo_topic (list)全部话题
        """
        self.weibo_topic = []
        for i in value:
            for j in i:
                print "id,时间,博文id,博文,点赞,转发链接,转发量,评论链接,评论量,新闻源", key[0], j[0], j[1], j[2], j[3], j[4], j[5], j[6], j[7], \
                    j[8]
                topic_patternts = re.compile('(【.*?】)')
                topic = topic_patternts.findall(j[2])
                if len(topic) > 0:
                    if topic[0] not in self.weibo_topic:
                        self.weibo_topic.append(topic[0])
                        print "话题", topic[0]
                        self.event_id = 'topic'+str(j[1])
                        Database().save_participate(key[0], self.event_id)
                        # print "搜索其他猎头......"
                        # if self.get_search_topic(key[0], topic[0]):
                        #     print "话题搜索完毕"
                        # else:
                        #     print "没有搜索到结果或者搜索的结果太少"
                    else:
                        print "该话题已经搜索完毕，不再进行搜索", topic[0]
                else:
                    print "这篇博文没有话题，无法搜索"

        for tp in self.weibo_topic:
            print "话题有：", tp
        return self.weibo_topic

    def get_search_topic(self, uid, topic):
        """
        此方法用来判断话题是否为热点
        :param topic: 要搜索的话题。str
        :return: 布尔值，是热点就返回true,不是热点就返回false
        """
        print "正在搜索话题: ", topic
        hot_url = 'http://weibo.cn/search/mblog/?keyword=' + str(
            urllib2.quote(topic)) + '&sort=hot'
        print 'hot', hot_url
        req = urllib2.Request(url=hot_url, headers=self.header)
        result_page = urllib2.urlopen(req).read()
        print "result", result_page

        if "抱歉，未找到" in result_page:
            req = urllib2.Request(url=hot_url, headers=self.header)
            result_page = urllib2.urlopen(req).read()
            # print "第二次尝试，刷新后", result_page
            if "抱歉，未找到" in result_page:
                print "看来真的没有结果了~~~"
                return False

        if "抱歉，未找到" not in result_page:
            result_num_patternts = re.compile('<span class="cmt">共(.*?)条</span>')
            result_num = result_num_patternts.findall(result_page)
            print 'rr',result_num
            print "result_num", result_num[0]

            if int(result_num[0]) > 1000:
                page_num = 50  # ps
            elif int(result_num[0]) <= 10:
                page_num = 0
            else:
                if int(result_num[0]) % 10 > 0:
                    page_num = int(result_num[0]) / 10 + 1
                else:
                    page_num = int(result_num[0]) / 10

            if page_num > 0:
                self.dynamic_dir = create_time_file(topic)  # 动态生成文件夹，全局变量
                print "生成的动态目录：", self.dynamic_dir
                corpus_dir = self.dynamic_dir + 'uid=' + str(uid) + '.txt'
                label_dir = BASE_DIR+'/documents/label_xls/1.xls'
                data_dir = label_dir
                txt_file = open(corpus_dir, 'w+')
                Database().save_network_scale(self.event_id,corpus_dir,data_dir,label_dir)
                big_v_num = 0
                print "有", result_num[0], "条结果,判断为热点新闻"
                like_all_num = 0
                forward_all_num = 0
                comment_all_num = 0
                for page in xrange(0, page_num):
                    page = int(page) + 1
                    pageurl = 'http://weibo.cn/search/mblog?keyword=' + str(
                        urllib2.quote(topic)) + '&sort=hot&page=' + str(page)
                    print pageurl

                    req = urllib2.Request(url=pageurl, headers=self.header)
                    result_turn_page = urllib2.urlopen(req).read()
                    # print page, "page", result_turn_page
                    issuer_patternts = re.compile('<div class="c" (id.*?)<div class="s"></div>')  # 把每个人的一个整个匹配下来
                    issuer = issuer_patternts.findall(result_turn_page)

                    print "该页大v 的个数为：", len(issuer)

                    for item in issuer:
                        if 'alt="V"/>' in item:
                            big_v_num += 1  # 记录大v的个数

                            issuer_all_patternts = re.compile(
                                'class="c" id="(.*?)">.*?<span class="ctt">(.*?)</span>.*?>赞\[(\d+)]</a>&nbsp;<a href="(.*?)">转发\[(\d+)]</a>&nbsp;<a href="(.*?)" class="cc">评论\[(\d+)]</a>.*?<span class="ct">(.*?)&nbsp'
                            )

                            issuer_id_name_patternts = re.compile(
                                '<a class="nk" href="http://weibo.cn/(.*?)">(.*?)</a>')  # 匹配一段里面的名字和id
                            issuer_id_name = issuer_id_name_patternts.findall(item)
                            clean_issuer_id_patternts = re.compile('u/')
                            cleaned_issuer_id_name = re.sub(clean_issuer_id_patternts, '', issuer_id_name[0][0])
                            print "大v博文用户id和名字", cleaned_issuer_id_name, issuer_id_name[0][1]

                            issuer_blog_id_patternts = re.compile('id="(.*?)">')
                            issuer_blog_id = issuer_blog_id_patternts.findall(item)
                            print "博文id", issuer_blog_id[0]

                            # issuer_blog_patternts = re.compile('<span class="ctt">(.*?)</span>')
                            issuer_blog_patternts = re.compile('<span class="ctt">:(.*?)>赞')
                            issuer_blog_unclean = issuer_blog_patternts.findall(item)
                            issuer_blog = self.cleaned_weibo(issuer_blog_unclean[0])
                            # clean_blog_patternts = re.compile('<.*?>')
                            # issuer_blog = re.sub(clean_blog_patternts, '', issuer_blog)
                            extra1 = re.compile('&nbsp;.*$')
                            cleaned_issuer_blog = re.sub(extra1, '', issuer_blog)
                            print "净化的博文", cleaned_issuer_blog

                            # 匹配点赞 转发 评论
                            like_forward_comment_patternts = re.compile(
                                '>赞\[(\d+)]</a>&nbsp;<a href="(.*?)">转发\[(\d+)]</a>&nbsp;<a href="(.*?)" class="cc">评论\[(\d+)]</a>')
                            like_forward_comment = like_forward_comment_patternts.findall(item)
                            print "点赞", like_forward_comment[0][0]
                            like_all_num += int(like_forward_comment[0][0])  # 计算总和，记录该话题的点赞规模
                            print "转发", like_forward_comment[0][2], like_forward_comment[0][1]
                            forward_all_num += int(like_forward_comment[0][2])  # 计算总和，记录该话题的转发规模
                            print "评论", like_forward_comment[0][4], like_forward_comment[0][3]
                            comment_all_num += int(like_forward_comment[0][4])  # 计算总和，记录该话题的评论规模
                            # 匹配时间
                            blog_time_paternts = re.compile('<!---->&nbsp;<span class="ct">(.*?)&nbsp;')
                            informality_blog_time = blog_time_paternts.findall(item)

                            if len(informality_blog_time) == 0:
                                blog_time_paternts1 = re.compile('<span class="ct">(.*?)</span>')
                                informality_blog_time = blog_time_paternts1.findall(item)

                            today_patternts = re.compile('今天')
                            minago_patternts = re.compile('\d+分钟前')
                            t = time.strftime('%m' + '月' + '%d' + '日', time.localtime())
                            t = t.decode('utf-8')
                            formality_blog_time = re.sub(today_patternts, t, informality_blog_time[0])
                            formality_blog_time = re.sub(minago_patternts, t, formality_blog_time)

                            try:
                                print "博文发表时间", formality_blog_time
                            except:
                                pass

                            # 爬取一条博文的转发路径，根据转发数
                            if int(like_forward_comment[0][2]) > 100:
                                forward_pages = 50  # ps

                            elif int(like_forward_comment[0][2]) <= 10:
                                forward_pages = 1

                            else:
                                if int(like_forward_comment[0][2]) % 10 > 0:
                                    forward_pages = int(like_forward_comment[0][2]) / 10 + 1
                                else:
                                    forward_pages = int(like_forward_comment[0][2]) / 10
                            reason_repeat_list = []
                            blog_origin = []
                            for forward_page in xrange(0, forward_pages):
                                f_page = forward_page + 1
                                forward_url = str(like_forward_comment[0][1]) + '&page=' + str(f_page)
                                print "转发链接", forward_url
                                reason_list = self.get_forward_common(forward_url)  # 爬取转发路径
                                reason_repeat_list += reason_list[0]
                                blog_origin.append(reason_list[1][0])
                            no_repeat_reason_list = list(set(reason_repeat_list))
                            no_repeat_reason_list.sort(key=reason_repeat_list.index)  # 去重后按时间顺序排序
                            print "新闻源：", blog_origin[0]

                            # 写入文本 txt_file
                            txt_file.write(
                                str(issuer_blog_id[0])  # '博文id：'
                                + '\n' + str(formality_blog_time)  # 博文时间
                                + '\n' + str(cleaned_issuer_blog)  # 博文
                                + '\n' + str(cleaned_issuer_id_name)  # 首发者的id
                                + '\n' + str(issuer_id_name[0][1])  # 首发者的名字
                                # + "\n点赞:" + like_forward_comment[0][0]  # 该博文的点赞量
                                # + "\n转发:" + like_forward_comment[0][2]  # 该博文的转发量
                                # + "\n评论:" + like_forward_comment[0][4]  # 该博文的评论量
                                + '\n' + str(blog_origin[0])
                                + '\n')

                            Database().save_content(str(issuer_blog_id[0]),str(formality_blog_time),self.event_id,str(cleaned_issuer_blog))
                            for reason in no_repeat_reason_list:  # 爬取转发路径，存储转发路径
                                print "转发理由：", reason
                                txt_file.write(str(reason) + '\n')
                            txt_file.write('\n')

                            # ---------------------------------
                        else:
                            issuer_id_name_patternts = re.compile(
                                '<a class="nk" href="http://weibo.cn/(.*?)">(.*?)</a>')  # 匹配一段里面的名字和id
                            issuer_id_name = issuer_id_name_patternts.findall(item)
                            print "非大v博文用户id和名字", issuer_id_name[0][0], issuer_id_name[0][1]

                print "该话题的点赞规模", like_all_num
                print "该话题的转发规模", forward_all_num
                print "该话题的评论规模", comment_all_num
                txt_file.write("\n-----------------"
                               + "\n该话题的点赞规模:" + str(like_all_num)
                               + "\n该话题的转发规模:" + str(forward_all_num)
                               + "\n该话题的评论规模:" + str(comment_all_num))

                txt_file.close()

                print "文本写入完毕"
                print "转发该话题的转发者是大的v个数：", big_v_num

                Database().save_increment(self.event_id, comment_all_num, forward_all_num, comment_all_num)


                return True
            else:
                print "搜索结果少于10个，直接忽略", hot_url
                return False

    def get_comment(self, key, value):
        """
        爬取每条微博的评论
        :return:comment
        """
        comment_file = open(self.comment_dir + 'uid=' + str(key[0]) + '.txt', 'w+')
        id_list = []
        user_name_list = []
        comment_list = []
        for i in value:
            for j in i:
                print "id,时间,博文id,博文,点赞,转发链接,转发量,评论链接,评论量", key[0], j[0], j[1], j[2], j[3], j[4], j[5], j[6], j[7]
                url = j[6]
                print "评论链接j[5]", url

                req = urllib2.Request(url=url, headers=self.header)
                comment_page = urllib2.urlopen(req).read()
                comment_patterns = re.compile(
                    '<div class="c" id="C.*?<a href="/u/(.*?)">(.*?)</a>.*?<span class="ctt">(.*?)</span>')

                comment = comment_patterns.findall(comment_page)

                print "博文为：", j[1] + ' ' + j[2]
                print "以下是该博文的评论"
                comment_file.write("博文id为： " + j[1] + ' ' + j[2] + '\n' + "以下是该博文的评论： " + '\n')
                for k in comment:
                    link_patterns = re.compile('<.*?>')
                    kk = re.sub(link_patterns, "", k[2])
                    print 'id:', k[0], '用户名:', k[1], '评论:', kk
                    # id_list.append(k[0])
                    # user_name_list.append(k[1])
                    # comment_list.append(kk)
            # id_name_comment = zip(id_list,user_name_list,comment_list)
                    comment_file.write(
                        "id: " + str(k[0]) + ', 用户名： ' + str(k[1]) + ', 评论： ' + str(kk) + '\n')

            comment_file.close()
        return True

    def get_forward_path(self, key, value):
        """
        爬取每条博文的转发路径
        :param key:
        :param value:
        :return:
        """
        for i in value:
            forward_path_file = open(self.forward_path_dir + 'uid=' + str(key[0]) + '.txt', 'w+')
            for j in i:
                print "id,时间,博文id,博文,点赞,转发链接,转发量,评论链接,评论量", key[0], j[0], j[1], j[2], j[3], j[4], j[5], j[6], j[7]
                url = j[4]
                print "博文为：", j[1] + ' ' + j[2]
                print "该转发链接： ", url
                req = urllib2.Request(url=url, headers=self.header)
                forward_page = urllib2.urlopen(req).read()

                # 通过原文评论链接找到原文转发链接，从而找到转发路径
                if "转发了" in forward_page:
                    original_comment_url_patterns = re.compile(
                        '<span class="cmt">原文转发.*?<a href="(.*?)" class="cc">原文评论\[(.*?)]')
                    original_comment_url = original_comment_url_patterns.findall(forward_page)
                    comment_url = "http://weibo.cn" + original_comment_url[0][0]
                    # print "该博文原文评论链接： ", comment_url

                    # 进入原文评论页面，找到转发的链接
                    req = urllib2.Request(url=comment_url, headers=self.header)
                    original_comment_page = urllib2.urlopen(req).read()
                    original_forward_url_patterns = re.compile(
                        '<a href="/repost(.*?);#rt">转发\[(.*?)]</a>')  # 匹配转发链接和转发数量
                    original_forward_url = original_forward_url_patterns.findall(original_comment_page)

                    # 匹配第一个转发博文的用户
                    first_forwarder_patterns = re.compile(
                        '<div class="c" id="M_"><div>    <a href=".*?">(.*?)</a>.*?<a href="/repost/.*?uid=(.*?)&amp;.*?转发')
                    first_forwarder = first_forwarder_patterns.findall(original_comment_page)
                    print first_forwarder
                    print "首发者的用户名,id： ", first_forwarder[0][0], first_forwarder[0][1]
                    forward_path_file.write(
                        "博文为： " + j[1] + ' ' + j[2] + '\n' + "首发者的用户名,id： " + first_forwarder[0][0] + "," +
                        first_forwarder[0][1] + '\n')
                    print "该博文原文转发链接： ", "http://weibo.cn/repost" + original_forward_url[0][0]
                    print "以下是该博文的转发人及转发理由："
                    if int(original_forward_url[0][1]) > 50:
                        repeat_list = []
                        blog_origin = []
                        print "转发量：", original_forward_url[0][1]
                        for page in xrange(1, int(original_forward_url[0][1]) / 10):  # 翻页
                            forward_url = "http://weibo.cn/repost" + original_forward_url[0][0] + "&page=" + str(page)
                            print forward_url
                            forward_string = self.get_forward_common(forward_url)
                            # print "调用时", len(forward_string)
                            repeat_list += forward_string[0]
                            # print "我就看看",len(repeat_list)
                            blog_origin.append(forward_string[1][0])

                        # print "大去重前", len(repeat_list)
                        no_repeat = list(set(repeat_list))
                        no_repeat.sort(key=repeat_list.index)
                        print "大去重后", len(no_repeat)
                        print "新闻源：", blog_origin[0]
                        forward_path_file.write(str(blog_origin[0]) + '\n')
                        for item in no_repeat:
                            print item
                            forward_path_file.write(item + '\n')

                else:
                    print "此条链接是首发链接"
                    first_forwarder_patterns = re.compile(
                        '<div class="c" id="M_"><div>    <a href=".*?">(.*?)</a>.*?<a href="/comment.*?&amp;uid=(.*?)&amp;#cmtfrm')
                    first_forwarder = first_forwarder_patterns.findall(forward_page)
                    print first_forwarder
                    print "首发者的用户名,id： ", first_forwarder[0][0] + " " + first_forwarder[0][1]
                    forward_path_file.write(
                        "博文为： " + j[1] + ' ' + j[2] + '\n' + "首发者的用户名,id： " + first_forwarder[0][0] + "," +
                        first_forwarder[0][1] + '\n')
                    print "转发量：", j[5]
                    if int(j[5]) > 50:
                        repeat_list = []
                        blog_origin = []
                        for page in xrange(1, int(j[5]) / 10):
                            forward_url = j[4] + "&page=" + str(page)
                            print forward_url
                            forward_string = self.get_forward_common(forward_url)
                            repeat_list += forward_string[0]
                            print "我就看看", len(repeat_list)

                            blog_origin.append(forward_string[1][0])

                        # print "大去重前", len(repeat_list)
                        no_repeat = list(set(repeat_list))
                        no_repeat.sort(key=repeat_list.index)
                        print "大去重后", len(no_repeat)
                        print "新闻源：", blog_origin[0]
                        forward_path_file.write(str(blog_origin[0]) + '\n')

                        for item in no_repeat:
                            print item
                            forward_path_file.write(item + '\n')
            forward_path_file.close()
        return True

    def get_forward_common(self, forward_url):
        """
        匹配转发路径及理由，用列表返回
        :param forward_url: 转发页的链接
        :return:该页的转发路径及理由
        """
        # 转发页需要用到的正则
        forward_patterns = re.compile(
            # '<div class="s"></div><div class="c"><a href="/u/(.*?)">(.*?)</a>.*?:(.*?)&nbsp'   # 匹配全部转发者
            '//<a href="/n.*?>(@.*?)</a>:(.*?)&nbsp;')  # 去除最后一个转发者的路径
        other_patterns = re.compile('//<.*?>')
        label_pattern = re.compile('<.*?>')

        req = urllib2.Request(url=forward_url, headers=self.header)
        forward_page = urllib2.urlopen(req).read()

        # print "help", forward_page
        forward_path = forward_patterns.findall(forward_page)

        blog_origin_patternts = re.compile('>(http://t.cn/.*?)</a></span>')
        useless = re.compile('</a>.*$')
        blog_origin = blog_origin_patternts.findall(forward_page)

        if len(blog_origin) > 0:
            blog_origin = [re.sub(useless,'',blog_origin[0])]

        if len(blog_origin) == 0:
            blog_origin.append(str(None))

        forward_string_list = []
        # print "forward_path", forward_path
        for k in forward_path:
            kk = re.sub(other_patterns, " ", k[1])
            forward_reason = re.sub(label_pattern, " ", kk)
            forward_string = str(k[0]) + ": " + str(forward_reason)
            forward_string_list.append(forward_string)
        # print "小去重前", len(forward_string_list)
        no_repeat_list = list(set(forward_string_list))
        no_repeat_list.sort(key=forward_string_list.index)
        # print "小去重后", len(no_repeat_list)
        return no_repeat_list, blog_origin  # 返回转发路径和博文源


if __name__ == '__main__':

    MoblieWeibo().login('734093894@qq.com', '18318210010')
    # 'odlmyfbw@sina.cn','tttt5555')#'1939777358@qq.com', '123456a')  # 734093894@qq.com   18826103742
    account = {
        "人民日报": 2803301701,
        "新浪新闻": 2028810631, "凤凰周刊": 1267454277,
        "网易新闻客户端": 1974808274, "北京晨报": 1646051850, "微博股票": 5578381471,
        "头条新闻": 1618051664, "cctv5": 2993049293, "人民网": 2286908003,
        "英国报姐": 3099016097,
        "财经网": 1642088277, "新京报": 1644114654, "环球时报": 1974576991,
        "中国新闻网": 1784473157, "三联生活周刊": 1191965271, "法制晚报": 1644948230,
        "新闻晨报": 1314608344, "中国之声": 1699540307
    }
    for name, acc in account.items():
        print "当前猎头：", name, acc
        weibo_page = WeiboPage(acc)

        a = multiprocessing.Process(target=weibo_page.get_page, args=(1, 2))
        a.start()
    a.join()
    print
'完成多进程'
